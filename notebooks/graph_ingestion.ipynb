{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from alive_progress import alive_bar\n",
    "from fastembed import TextEmbedding\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from llama_index.core import Settings, Document, PropertyGraphIndex \n",
    "from llama_index.core.node_parser import MarkdownElementNodeParser\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.extractors.relik.base import RelikPathExtractor\n",
    "from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\n",
    "from llama_index.llms.anthropic import Anthropic\n",
    "from llama_parse import LlamaParse\n",
    "from typing import List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "CLAUDE_API_KEY = os.getenv('CLAUDE_API_KEY')\n",
    "LLAMA_API_KEY = os.getenv('LLAMA_API_KEY')\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = CLAUDE_API_KEY\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = LLAMA_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=4096,\n",
    "    temperature=0.0,\n",
    "    stop=[\"\\n\\nHuman\"],\n",
    ")\n",
    "\n",
    "llama_llm = Anthropic(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=4096,\n",
    "    temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11cf787c2c54f2f9b621305d956f65a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bge_embed_model = TextEmbedding(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "llama_openai_embed_model = OpenAIEmbedding(model_name=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                ___              __         \n",
      "               /\\_ \\      __    /\\ \\        \n",
      " _ __     __   \\//\\ \\    /\\_\\   \\ \\ \\/'\\    \n",
      "/\\`'__\\ /'__`\\   \\ \\ \\   \\/\\ \\   \\ \\ , <    \n",
      "\\ \\ \\/ /\\  __/    \\_\\ \\_  \\ \\ \\   \\ \\ \\\\`\\  \n",
      " \\ \\_\\ \\ \\____\\   /\\____\\  \\ \\_\\   \\ \\_\\ \\_\\\n",
      "  \\/_/  \\/____/   \\/____/   \\/_/    \\/_/\\/_/\n",
      "                                            \n",
      "                                            \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2024-10-14 18:50:10,064] [WARNING] [relik.common.utils.download_from_hf:342] [PID:36853] [RANK:0] Couldn't download index.faiss from relik-ie/encoder-e5-small-v2-wikipedia-relations-index, ignoring\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/relik/retriever/indexers/base.py:534: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embeddings = torch.load(embedding_path, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "relik = RelikPathExtractor(\n",
    "    model=\"relik-ie/relik-relation-extraction-small\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_lg' (3.5.0) was trained with spaCy v3.5.0 and may not be 100% compatible with the current version (3.7.6). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<coreferee.manager.CorefereeBroker at 0x3d7999150>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coref_nlp = spacy.load('en_core_web_lg')\n",
    "coref_nlp.add_pipe('coreferee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate doc parser\n",
    "parser = LlamaParse(\n",
    "    result_type=\"markdown\",\n",
    "    num_workers=8,\n",
    "    verbose = False,\n",
    "    show_progress=True,\n",
    "    ignore_errors=True,\n",
    "    language=\"en\",\n",
    ")\n",
    "\n",
    "# instantiate node parser\n",
    "node_parser = MarkdownElementNodeParser(llm=llama_llm, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LONG_CHUNK_SIZE = 2000\n",
    "\n",
    "def coref_text(text):\n",
    "    coref_doc = coref_nlp(text.strip())\n",
    "    resolved_text = \"\"\n",
    "\n",
    "    for token in coref_doc:\n",
    "        repres = coref_doc._.coref_chains.resolve(token)\n",
    "        if repres:\n",
    "            resolved_text += \" \" + \" and \".join(\n",
    "                [\n",
    "                    t.text\n",
    "                    if t.ent_type_ == \"\"\n",
    "                    else [e.text for e in coref_doc.ents if t in e][0]\n",
    "                    for t in repres\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            resolved_text += \" \" + token.text\n",
    "\n",
    "    return resolved_text.strip()\n",
    "\n",
    "def remove_table_of_contents(text):\n",
    "    pattern = r\"TABLE OF CONTENTS.*?(?=#)\"\n",
    "    cleaned_text = re.sub(pattern, \"\", text, flags=re.DOTALL)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "def convert_nodes_to_documents(text_nodes, object_nodes, source):\n",
    "    \"\"\"\n",
    "    Converts nodes to Documents\n",
    "\n",
    "    Args:\n",
    "        text_nodes (List[Nodes]): List of text nodes\n",
    "        object_nodes (List[Nodes]): List of object nodes\n",
    "        source (str): Source of the document\n",
    "\n",
    "    Returns:\n",
    "        documents (List[Documents]): List of Documents\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for node in text_nodes:\n",
    "        text = coref_text(node.text)\n",
    "        doc = Document(\n",
    "            text= text,\n",
    "            metadata = {\n",
    "                \"is_table\": False,\n",
    "                \"source\": source\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "        \n",
    "    for node in object_nodes:\n",
    "        text = coref_text(node.text)\n",
    "        doc = Document(\n",
    "            text= text,\n",
    "            metadata = {\n",
    "                \"is_table\": True,\n",
    "                \"source\": source\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "        \n",
    "    return documents\n",
    "\n",
    "def make_dir(data_folder):\n",
    "    os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "def parse_docs(file_location: str, data_folder: Optional[str] = None) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Parses PDF Folder and returns a list of Documents\n",
    "\n",
    "    Args:\n",
    "        file_location (str): PDF Folder Location\n",
    "        data_folder (Optional[str], optional): Folder to save pickles (Optional). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: _description_\n",
    "    \"\"\"\n",
    "    all_docs = []\n",
    "    for file_name in os.listdir(file_location):\n",
    "        if not file_name.endswith(\".pdf\"):\n",
    "            continue\n",
    "\n",
    "        print(\"File: \" + str(file_name))\n",
    "        doc_path = os.path.join(file_location, file_name)\n",
    "        modified_file_name = os.path.splitext(file_name)[0].lower().replace(' ', '_')\n",
    "\n",
    "        # results in a list of Document Objects\n",
    "        documents = parser.load_data(doc_path)\n",
    "        \n",
    "        for idx, doc in enumerate(documents):\n",
    "            doc.text = remove_table_of_contents(doc.text)\n",
    "            if idx > 4:\n",
    "                break\n",
    "\n",
    "        raw_nodes = node_parser.get_nodes_from_documents(documents)\n",
    "        # list of text_nodes, list of objects\n",
    "        text_nodes, objects = node_parser.get_nodes_and_objects(raw_nodes)\n",
    "        \n",
    "        final_docs = convert_nodes_to_documents(text_nodes, objects, modified_file_name)\n",
    "        all_docs.append(final_docs)\n",
    "        \n",
    "        if data_folder:\n",
    "            data_path = os.path.join(data_folder, modified_file_name + '.pkl')\n",
    "            pickle.dump(final_docs, open(data_path, \"wb\"))\n",
    "    \n",
    "    return [item for sublist in all_docs for item in sublist]\n",
    "\n",
    "def read_pickles(data_folder: str) -> List[Document]:\n",
    "    doc_list = []\n",
    "    for file_name in os.listdir(data_folder):\n",
    "        doc_path = os.path.join(data_folder, file_name)\n",
    "        if file_name.endswith(\".pkl\"):\n",
    "            with open(doc_path, 'rb') as file:\n",
    "                # data will be a doc_list\n",
    "                data = pickle.load(file)\n",
    "                doc_list.append(data)\n",
    "                \n",
    "    # since doc_list is a list of list of documents, we need to flatten it\n",
    "    doc_list = [item for sublist in doc_list for item in sublist]\n",
    "    return doc_list\n",
    "\n",
    "def further_split_long_docs(doc_list: List[Document]) -> Tuple[List[Document], List[Document]]:\n",
    "    long_docs, short_docs = [], []\n",
    "    for doc in doc_list:\n",
    "        is_table = doc.metadata[\"is_table\"]\n",
    "        if not is_table:\n",
    "            if len(doc.text) > LONG_CHUNK_SIZE:\n",
    "                long_docs.append(doc)\n",
    "            else:\n",
    "                short_docs.append(doc)\n",
    "        else:\n",
    "            short_docs.append(doc)\n",
    "    return long_docs, short_docs\n",
    "                \n",
    "def chunk_doc(doc: Document, text_splitter: RecursiveCharacterTextSplitter) -> List[Document]:\n",
    "    chunks = text_splitter.split_text(doc.text)\n",
    "    return [\n",
    "        Document(\n",
    "            text=chunk,\n",
    "            metadata={\n",
    "                'is_table': doc.metadata['is_table'],\n",
    "                'source': doc.metadata.get('source', '')\n",
    "            }\n",
    "        )\n",
    "        for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    \n",
    "def recursive_chunk_documents(long_docs: List[Document],\n",
    "                              short_docs: List[Document], \n",
    "                              chunk_size: int = 1024, \n",
    "                              chunk_overlap: int = 128,\n",
    "                              separators: List[str] = [\"\\n\\n\", \"\\n\", \" \", \"\"]) -> List[Document]:\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=separators\n",
    "    )\n",
    "\n",
    "    for doc in long_docs:\n",
    "        short_docs.extend(chunk_doc(doc, text_splitter))\n",
    "\n",
    "    return short_docs\n",
    "\n",
    "def get_final_docs(data_folder: Optional[str] = None, doc_list: Optional[List[Document]] = None) -> List[Document]:\n",
    "    if doc_list is None:\n",
    "        if data_folder is None:\n",
    "            raise ValueError(\"Either data_folder or doc_list must be provided\")\n",
    "        doc_list = read_pickles(data_folder)\n",
    "    \n",
    "    long_docs, short_docs = further_split_long_docs(doc_list)\n",
    "    final_docs = recursive_chunk_documents(long_docs, short_docs)\n",
    "    return final_docs\n",
    "        \n",
    "def parse_and_process_docs(file_location, data_folder: Optional[str] = None) -> List[Document]:\n",
    "    if data_folder:\n",
    "        make_dir(data_folder)\n",
    "        all_docs = parse_docs(file_location=file_location, data_folder=data_folder)\n",
    "    else:\n",
    "        all_docs = parse_docs(file_location=file_location)\n",
    "        \n",
    "    final_docs = get_final_docs(doc_list=all_docs)\n",
    "    return final_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: Diabetes Medications.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00, 34663.67it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: managing-pre-diabetes-(updated-on-27-jul-2021)c2bfc77474154c2abf623156a4b93002.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "1it [00:00, 25731.93it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, 11781.75it/s]\n",
      "3it [00:00, 39945.75it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: Diabetic Foot Ulcer_ Symptoms and Treatment.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: Diabetes Treatment_ Insulin.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, 20460.02it/s]\n"
     ]
    }
   ],
   "source": [
    "final_docs = parse_and_process_docs(file_location=\"pdfs\")\n",
    "pickle.dump(final_docs, open('data/final_docs.pkl', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"15082001\"\n",
    "NEO4J_DATABASE = \"neo4j\"\n",
    "\n",
    "graph_store = Neo4jPropertyGraphStore(\n",
    "    username=NEO4J_USER,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    url=NEO4J_URI,\n",
    "    refresh_schema=False,\n",
    ")\n",
    "\n",
    "# gds = GraphDataScience(NEO4J_URI, database=NEO4J_DATABASE, auth=(NEO4J_USER, NEO4J_PASSWORD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_all_nodes(graph_store):\n",
    "    graph_store.structured_query(\"\"\"\n",
    "    MATCH (n)\n",
    "    DETACH DELETE n\n",
    "    \"\"\")\n",
    "    print(\"All nodes deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All nodes deleted\n"
     ]
    }
   ],
   "source": [
    "delete_all_nodes(graph_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_neo4j_restrictions(graph_store):\n",
    "    graph_store.structured_query(\"\"\"\n",
    "    CALL apoc.schema.assert({}, {});\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 11/11 [00:00<00:00, 3909.28it/s]\n",
      "Extracting triples: 100%|██████████| 11/11 [02:43<00:00, 14.84s/it]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.52s/it]\n"
     ]
    }
   ],
   "source": [
    "index = PropertyGraphIndex.from_documents(\n",
    "    final_docs,\n",
    "    kg_extractors=[relik],\n",
    "    llm=llama_llm,\n",
    "    embed_model=llama_openai_embed_model,\n",
    "    property_graph_store=graph_store,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph De-duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_index(graph_store):\n",
    "    graph_store.structured_query(\"\"\"\n",
    "    CREATE VECTOR INDEX entity IF NOT EXISTS\n",
    "    FOR (m:`__Entity__`)\n",
    "    ON m.embedding\n",
    "    OPTIONS {indexConfig: {\n",
    "    `vector.dimensions`: 1536,\n",
    "    `vector.similarity_function`: 'cosine'\n",
    "    }}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_vector_index(graph_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_graph_deduplication(graph_store, similarity_threshold = 0.90, word_edit_distance = 5):\n",
    "    data = graph_store.structured_query(\"\"\"\n",
    "    MATCH (e:__Entity__)\n",
    "    CALL {\n",
    "    WITH e\n",
    "    CALL db.index.vector.queryNodes('entity', 10, e.embedding)\n",
    "    YIELD node, score\n",
    "    WITH node, score\n",
    "    WHERE score > toFLoat($cutoff)\n",
    "        AND (toLower(node.name) CONTAINS toLower(e.name) OR toLower(e.name) CONTAINS toLower(node.name)\n",
    "            OR apoc.text.distance(toLower(node.name), toLower(e.name)) < $distance)\n",
    "        AND labels(e) = labels(node)\n",
    "    WITH node, score\n",
    "    ORDER BY node.name\n",
    "    RETURN collect(node) AS nodes\n",
    "    }\n",
    "    WITH distinct nodes\n",
    "    WHERE size(nodes) > 1\n",
    "    WITH collect([n in nodes | n.name]) AS results\n",
    "    UNWIND range(0, size(results)-1, 1) as index\n",
    "    WITH results, index, results[index] as result\n",
    "    WITH apoc.coll.sort(reduce(acc = result, index2 IN range(0, size(results)-1, 1) |\n",
    "            CASE WHEN index <> index2 AND\n",
    "                size(apoc.coll.intersection(acc, results[index2])) > 0\n",
    "                THEN apoc.coll.union(acc, results[index2])\n",
    "                ELSE acc\n",
    "            END\n",
    "    )) as combinedResult\n",
    "    WITH distinct(combinedResult) as combinedResult\n",
    "    // extra filtering\n",
    "    WITH collect(combinedResult) as allCombinedResults\n",
    "    UNWIND range(0, size(allCombinedResults)-1, 1) as combinedResultIndex\n",
    "    WITH allCombinedResults[combinedResultIndex] as combinedResult, combinedResultIndex, allCombinedResults\n",
    "    WHERE NOT any(x IN range(0,size(allCombinedResults)-1,1) \n",
    "        WHERE x <> combinedResultIndex\n",
    "        AND apoc.coll.containsAll(allCombinedResults[x], combinedResult)\n",
    "    )\n",
    "    RETURN combinedResult  \n",
    "    \"\"\", param_map={'cutoff': similarity_threshold, 'distance': word_edit_distance})\n",
    "    for row in data:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'combinedResult': ['Glibenclamide', 'Gliclazide']}\n",
      "{'combinedResult': ['blood sugar', 'blood sugar level', 'blood sugar levels']}\n",
      "{'combinedResult': ['Alcohol', 'alcohol']}\n",
      "{'combinedResult': ['Anti - diabetes', 'Anti - diabetes Tablets', 'Diabetes', 'Diabetes Mellitus', 'anti - diabetes', 'diabetes', 'diabetic', 'type 2 diabetes']}\n"
     ]
    }
   ],
   "source": [
    "check_graph_deduplication(graph_store, similarity_threshold = 0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'combinedResult': ['blood sugar', 'blood sugar level', 'blood sugar levels']}\n",
      "{'combinedResult': ['Alcohol', 'alcohol']}\n",
      "{'combinedResult': ['Diabetes', 'Diabetes Mellitus', 'diabetes', 'diabetic', 'type 2 diabetes']}\n",
      "{'combinedResult': ['Anti - diabetes', 'Anti - diabetes Tablets', 'anti - diabetes']}\n"
     ]
    }
   ],
   "source": [
    "check_graph_deduplication(graph_store, similarity_threshold = 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_deduplication(graph_store, similarity_threshold = 0.90, word_edit_distance = 5):\n",
    "    graph_store.structured_query(\"\"\"\n",
    "        MATCH (e:__Entity__)\n",
    "        CALL {\n",
    "        WITH e\n",
    "        CALL db.index.vector.queryNodes('entity', 10, e.embedding)\n",
    "        YIELD node, score\n",
    "        WITH node, score\n",
    "        WHERE score > toFLoat($cutoff)\n",
    "            AND (toLower(node.name) CONTAINS toLower(e.name) OR toLower(e.name) CONTAINS toLower(node.name)\n",
    "                OR apoc.text.distance(toLower(node.name), toLower(e.name)) < $distance)\n",
    "            AND labels(e) = labels(node)\n",
    "        WITH node, score\n",
    "        ORDER BY node.name\n",
    "        RETURN collect(node) AS nodes\n",
    "        }\n",
    "        WITH distinct nodes\n",
    "        WHERE size(nodes) > 1\n",
    "        WITH collect([n in nodes | n.name]) AS results\n",
    "        UNWIND range(0, size(results)-1, 1) as index\n",
    "        WITH results, index, results[index] as result\n",
    "        WITH apoc.coll.sort(reduce(acc = result, index2 IN range(0, size(results)-1, 1) |\n",
    "                CASE WHEN index <> index2 AND\n",
    "                    size(apoc.coll.intersection(acc, results[index2])) > 0\n",
    "                    THEN apoc.coll.union(acc, results[index2])\n",
    "                    ELSE acc\n",
    "                END\n",
    "        )) as combinedResult\n",
    "        WITH distinct(combinedResult) as combinedResult\n",
    "        // extra filtering\n",
    "        WITH collect(combinedResult) as allCombinedResults\n",
    "        UNWIND range(0, size(allCombinedResults)-1, 1) as combinedResultIndex\n",
    "        WITH allCombinedResults[combinedResultIndex] as combinedResult, combinedResultIndex, allCombinedResults\n",
    "        WHERE NOT any(x IN range(0,size(allCombinedResults)-1,1)\n",
    "            WHERE x <> combinedResultIndex\n",
    "            AND apoc.coll.containsAll(allCombinedResults[x], combinedResult)\n",
    "        )\n",
    "        CALL {\n",
    "        WITH combinedResult\n",
    "            UNWIND combinedResult AS name\n",
    "            MATCH (e:__Entity__ {name:name})\n",
    "            WITH e\n",
    "            ORDER BY size(e.name) DESC // prefer longer names to remain after merging\n",
    "            RETURN collect(e) AS nodes\n",
    "        }\n",
    "        CALL apoc.refactor.mergeNodes(nodes, {properties: {\n",
    "            `.*`: 'discard'\n",
    "        }})\n",
    "        YIELD node\n",
    "        RETURN count(*)\n",
    "        \"\"\", param_map={'cutoff': similarity_threshold, 'distance': word_edit_distance}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_deduplication(graph_store, similarity_threshold = 0.95)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
