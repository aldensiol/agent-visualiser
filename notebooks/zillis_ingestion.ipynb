{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "from alive_progress import alive_bar\n",
    "from dotenv import load_dotenv\n",
    "from fastembed import TextEmbedding\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import MarkdownElementNodeParser\n",
    "from llama_index.llms.anthropic import Anthropic\n",
    "from llama_parse import LlamaParse\n",
    "from pymilvus import (\n",
    "    utility,\n",
    "    CollectionSchema, DataType, FieldSchema, model,\n",
    "    connections, Collection, AnnSearchRequest, WeightedRanker, RRFRanker,\n",
    ")\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in of API Keys and Cloud Infrastructure\n",
    "1. OpenAI (potentially for embedding models), Anthropic (for Claude 3.5 Sonnet), LLama (for LlamaParse)\n",
    "2. Zillis' Endpoint and Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "CLAUDE_API_KEY = os.getenv('CLAUDE_API_KEY')\n",
    "LLAMA_API_KEY = os.getenv('LLAMA_API_KEY')\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = CLAUDE_API_KEY\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = LLAMA_API_KEY\n",
    "\n",
    "ENDPOINT = os.getenv('ZILLIS_ENDPOINT')\n",
    "TOKEN = os.getenv('ZILLIS_TOKEN')\n",
    "\n",
    "connections.connect(uri=ENDPOINT, token=TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=4096,\n",
    "    temperature=0.0,\n",
    "    stop=[\"\\n\\nHuman\"],\n",
    ")\n",
    "\n",
    "llama_llm = Anthropic(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=4096,\n",
    "    temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collection Creation\n",
    "1. Drop existing collection (if one exists)\n",
    "2. Define Schema -> How your documents will be Ingested\n",
    "3. Create Collection with Schema defined in 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'vector_index' has been dropped\n"
     ]
    }
   ],
   "source": [
    "# Specify the collection name\n",
    "collection_name = \"vector_index\"\n",
    "\n",
    "def drop_collection(collection_name):\n",
    "    # Check if the collection exists\n",
    "    if utility.has_collection(collection_name):\n",
    "        collection = Collection(name=collection_name)\n",
    "        # Release the collection\n",
    "        collection.release()\n",
    "        # Drop the collection if it exists\n",
    "        utility.drop_collection(collection_name)\n",
    "        print(f\"Collection '{collection_name}' has been dropped\")\n",
    "    else:\n",
    "        print(f\"Collection '{collection_name}' does not exist\")\n",
    "\n",
    "drop_collection(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_id = FieldSchema(\n",
    "    name=\"pk\",\n",
    "    dtype=DataType.INT64,\n",
    "    is_primary=True,\n",
    "    auto_id=True)\n",
    "\n",
    "doc_id = FieldSchema(\n",
    "    name=\"doc_id\",\n",
    "    dtype=DataType.VARCHAR,\n",
    "    max_length=500\n",
    ")\n",
    "\n",
    "doc_source = FieldSchema(\n",
    "    name=\"doc_source\",\n",
    "    dtype=DataType.VARCHAR,\n",
    "    max_length=1000,\n",
    "    default_value=\"NA\"\n",
    ")\n",
    "\n",
    "doc_content = FieldSchema(\n",
    "    name=\"text\",\n",
    "    dtype=DataType.VARCHAR,\n",
    "    max_length=50000,\n",
    "    default_value=\"\"\n",
    ")\n",
    "\n",
    "vec_embeddings = FieldSchema(\n",
    "    name=\"dense_embeddings\",\n",
    "    dtype=DataType.FLOAT_VECTOR,\n",
    "    dim=1024\n",
    ")\n",
    "\n",
    "keyword_embeddings = FieldSchema(\n",
    "    name=\"sparse_embeddings\",\n",
    "    dtype=DataType.SPARSE_FLOAT_VECTOR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = CollectionSchema(\n",
    "  fields=[auto_id, doc_id, doc_content, doc_source, vec_embeddings, keyword_embeddings],\n",
    "  description=\"milvus_schema\",\n",
    "  enable_dynamic_field=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_collection(collection_name, schema):\n",
    "    # Check if the collection exists\n",
    "    if utility.has_collection(collection_name):\n",
    "        print(f\"Collection '{collection_name}' already exists\")\n",
    "    # Create the collection\n",
    "    return Collection(name=collection_name, schema=schema, using='default', shards_num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = create_collection(collection_name, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bge_embed_model = TextEmbedding(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "openai_embed_model = OpenAIEmbeddings(model_name=\"text-embedding-3-large\")\n",
    "spalde_embed_model = model.sparse.SpladeEmbeddingFunction(\n",
    "    model_name=\"naver/splade-cocondenser-ensembledistil\",\n",
    "    device=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "1. Defining Helper functions for cleaning\n",
    "2. Data Parsing using `LLamaParse` -> Node Parsing using MarkdownElementNodeParser. After this stage, Document objects are pickled\n",
    "3. Chunk long texts using appropriate technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_table_of_contents(text):\n",
    "    pattern = r\"TABLE OF CONTENTS.*?(?=#)\"\n",
    "    cleaned_text = re.sub(pattern, \"\", text, flags=re.DOTALL)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "# Reset the index whenever you want to re-index the documents\n",
    "idx = [0]\n",
    "\n",
    "def convert_nodes_to_documents(text_nodes, object_nodes, source):\n",
    "    \"\"\"\n",
    "    Converts nodes to Documents\n",
    "\n",
    "    Args:\n",
    "        text_nodes (List[Nodes]): List of text nodes\n",
    "        object_nodes (List[Nodes]): List of object nodes\n",
    "        source (str): Source of the document\n",
    "\n",
    "    Returns:\n",
    "        documents (List[Documents]): List of Documents\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for node in text_nodes:\n",
    "        text = node.text\n",
    "        doc_id = str(uuid.uuid4())\n",
    "        doc = Document(\n",
    "            text= text,\n",
    "            metadata = {\n",
    "                \"id\": f\"node_{idx[0]}_{doc_id}\",\n",
    "                \"start_char_idx\": node.start_char_idx,\n",
    "                \"end_char_idx\": node.end_char_idx,\n",
    "                \"is_table\": False,\n",
    "                \"source\": source\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "        idx[0] += 1\n",
    "        \n",
    "    for node in object_nodes:\n",
    "        text = node.text\n",
    "        doc_id = str(uuid.uuid4())\n",
    "        doc = Document(\n",
    "            text= text,\n",
    "            metadata = {\n",
    "                \"id\": f\"node_{idx[0]}_{doc_id}\",\n",
    "                \"start_char_idx\": node.start_char_idx,\n",
    "                \"end_char_idx\": node.end_char_idx,\n",
    "                \"is_table\": True,\n",
    "                \"source\": source\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "        idx[0] += 1\n",
    "        \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate doc parser\n",
    "parser = LlamaParse(\n",
    "    result_type=\"markdown\",\n",
    "    num_workers=8,\n",
    "    verbose = True,\n",
    "    language=\"en\",\n",
    ")\n",
    "\n",
    "# instantiate node parser\n",
    "node_parser = MarkdownElementNodeParser(llm=llama_llm, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder to store parsed data\n",
    "data_folder = \"data\"\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "def parse_docs(file_location):\n",
    "    for file_name in os.listdir(file_location):\n",
    "        if not file_name.endswith(\".pdf\"):\n",
    "            continue\n",
    "\n",
    "        print(\"File: \" + str(file_name))\n",
    "        doc_path = os.path.join(file_location, file_name)\n",
    "        modified_file_name = os.path.splitext(file_name)[0].lower().replace(' ', '_')\n",
    "        data_path = os.path.join(data_folder, modified_file_name + '.pkl')\n",
    "\n",
    "        # results in a list of Document Objects\n",
    "        documents = parser.load_data(doc_path)\n",
    "        \n",
    "        for idx, doc in enumerate(documents):\n",
    "            doc.text = remove_table_of_contents(doc.text)\n",
    "            if idx > 4:\n",
    "                break\n",
    "\n",
    "        raw_nodes = node_parser.get_nodes_from_documents(documents)\n",
    "        # list of text_nodes, list of objects\n",
    "        text_nodes, objects = node_parser.get_nodes_and_objects(raw_nodes)\n",
    "        \n",
    "        final_docs = convert_nodes_to_documents(text_nodes, objects, modified_file_name)\n",
    "\n",
    "        pickle.dump(final_docs, open(data_path, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location = \"pdfs\"\n",
    "\n",
    "# parse_docs(file_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading of pickles after parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickles(data_folder):\n",
    "    doc_list = []\n",
    "    for file_name in os.listdir(data_folder):\n",
    "        doc_path = os.path.join(data_folder, file_name)\n",
    "        if file_name.endswith(\".pkl\"):\n",
    "            with open(doc_path, 'rb') as file:\n",
    "                # data will be a doc_list\n",
    "                data = pickle.load(file)\n",
    "                doc_list.append(data)\n",
    "                \n",
    "    # since doc_list is a list of list of documents, we need to flatten it\n",
    "    doc_list = [item for sublist in doc_list for item in sublist]\n",
    "    return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = read_pickles(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive splitting for longer texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_docs, short_docs = [], []\n",
    "for doc in doc_list:\n",
    "    is_table = doc.metadata[\"is_table\"]\n",
    "    if not is_table:\n",
    "        if len(doc.text) > 1500:\n",
    "            long_docs.append(doc)\n",
    "        else:\n",
    "            short_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_doc(doc: Document, text_splitter: RecursiveCharacterTextSplitter) -> List[Document]:\n",
    "    chunks = text_splitter.split_text(doc.text)\n",
    "    return [\n",
    "        Document(\n",
    "            text=chunk,\n",
    "            metadata={\n",
    "                **doc.metadata,\n",
    "                'chunk_id': f\"{doc.metadata.get('id', '')}_{i}\",\n",
    "                'parent_id': doc.metadata.get('id', ''),\n",
    "            }\n",
    "        )\n",
    "        for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "\n",
    "def recursive_chunk_documents(long_docs: List[Document],\n",
    "                              short_docs: List[Document], \n",
    "                              chunk_size: int = 512, \n",
    "                              chunk_overlap: int = 64,\n",
    "                              separators: List[str] = [\"\\n\\n\", \"\\n\", \" \", \"\"]) -> List[Document]:\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=separators\n",
    "    )\n",
    "\n",
    "    for doc in long_docs:\n",
    "        short_docs.extend(chunk_doc(doc, text_splitter))\n",
    "\n",
    "    return short_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_docs = recursive_chunk_documents(long_docs, short_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids, all_texts, all_sources = [], [], []\n",
    "\n",
    "with alive_bar(len(final_docs), title='Metadata', force_tty=True) as bar:\n",
    "    for doc in final_docs:\n",
    "        try:\n",
    "            doc_id = doc.metadata['chunk_id']\n",
    "        except:\n",
    "            doc_id = doc.metadata['id']\n",
    "        text = doc.text\n",
    "        source = doc.metadata['source']\n",
    "        \n",
    "        all_ids.append(doc_id)\n",
    "        all_texts.append(text)\n",
    "        all_sources.append(source)\n",
    "        bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding texts (and tables) into respective Embedding Models\n",
    "1. Dense Embeddings with BGE\n",
    "2. Sparse Embeddings with SPLADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_embeddings_list = list(bge_embed_model.embed(all_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_embeddings_list = spalde_embed_model.encode_documents(all_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving of respective embedding lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_ids), len(all_texts), len(all_sources), len(dense_embeddings_list), sparse_embeddings_list.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/dense_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(dense_embeddings_list, f)\n",
    "    \n",
    "with open('data/sparse_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(sparse_embeddings_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    all_ids,\n",
    "    all_texts,\n",
    "    all_sources,\n",
    "    dense_embeddings_list,\n",
    "    sparse_embeddings_list\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size\n",
    "batch_size = 10\n",
    "total_elements = len(data[0])  # All lists have the same length\n",
    "\n",
    "# Calculate the total number of batches\n",
    "total_batches = (total_elements + batch_size - 1) // batch_size\n",
    "\n",
    "# Initialize alive_bar with the total number of batches\n",
    "with alive_bar(total_batches, force_tty=True) as bar:\n",
    "    for start in range(0, total_elements, batch_size):\n",
    "        end = min(start + batch_size, total_elements)\n",
    "        batch = [sublist[start:end] for sublist in data]\n",
    "        collection.insert(batch)\n",
    "        bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating of Index\n",
    "1. Delete any existing Index\n",
    "2. Create new Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_indexes(collection: Collection, index_names: List[str]) -> None:\n",
    "    # Release or drop the existing collection index\n",
    "    collection.release()\n",
    "    for name in index_names:\n",
    "        collection.drop_index(index_name=name)\n",
    "        print(f\"Index '{name}' has been dropped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_indexes(collection, index_names=[\"sparse_embeddings\", \"dense_embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the indexes\n",
    "def create_all_indexes(collection: Collection):\n",
    "    # Dense embeddings index\n",
    "    collection.create_index(\n",
    "        field_name=\"dense_embeddings\",\n",
    "        index_params={\n",
    "            \"metric_type\": \"COSINE\",\n",
    "            \"index_type\": \"HNSW\",\n",
    "            \"params\": {\n",
    "                \"M\": 5,\n",
    "                \"efConstruction\": 512\n",
    "            }\n",
    "        },\n",
    "        index_name=\"dense_embeddings_index\"\n",
    "    )\n",
    "    \n",
    "    print(\"Dense embeddings index created\")\n",
    "\n",
    "    # Sparse embeddings index\n",
    "    collection.create_index(\n",
    "        field_name=\"sparse_embeddings\",\n",
    "        index_params={\n",
    "            \"metric_type\": \"IP\",\n",
    "            \"index_type\": \"SPARSE_INVERTED_INDEX\",\n",
    "            \"params\": {\n",
    "                \"drop_ratio_build\": 0.2\n",
    "            }\n",
    "        },\n",
    "        index_name=\"sparse_embeddings_index\"\n",
    "    )\n",
    "    \n",
    "    print(\"Sparse embeddings index created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_all_indexes(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in of collection and setting up Hybrid Searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = Collection(name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query: str) -> str:\n",
    "    dense_embedding = list(bge_embed_model.query_embed(query))[0]\n",
    "    sparse_embedding = list(spalde_embed_model.encode_queries([query]))\n",
    "    \n",
    "    search_results = collection.hybrid_search(\n",
    "            reqs=[\n",
    "                AnnSearchRequest(\n",
    "                    data=[dense_embedding],  # content vector embedding\n",
    "                    anns_field='dense_embeddings',  # content vector field\n",
    "                    param={\"metric_type\": \"COSINE\", \"params\": {\"M\": 64, \"efConstruction\": 512}}, # Search parameters\n",
    "                    limit=3\n",
    "                ),\n",
    "                AnnSearchRequest(\n",
    "                    data=list(sparse_embedding),  # keyword vector embedding\n",
    "                    anns_field='sparse_embeddings',  # keyword vector field\n",
    "                    param={\"metric_type\": \"IP\", \"params\": {\"drop_ratio_build\": 0.2}}, # Search parameters\n",
    "                    limit=3\n",
    "                )\n",
    "            ],\n",
    "            output_fields=['doc_id', 'text', 'doc_source'],\n",
    "            # Use WeightedRanker to combine results with specified weights\n",
    "            # Alternatively, use RRFRanker for reciprocal rank fusion reranking\n",
    "            rerank=RRFRanker(),\n",
    "            limit=3\n",
    "            )\n",
    "    \n",
    "    hits = search_results[0]\n",
    "    \n",
    "    context = []\n",
    "    for res in hits:\n",
    "        text = res.text\n",
    "        source = res.doc_source\n",
    "        context.append(f\"Source: {source}\\nContext: {text}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = hybrid_search(\"How should i know whether i should throw away my insulin?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
