{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import boto3\n",
    "import logging\n",
    "import nest_asyncio\n",
    "import operator\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "from botocore.config import Config\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, HTML\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.anthropic import Anthropic\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_core.runnables import Runnable, RunnablePassthrough\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.tools import StructuredTool\n",
    "from typing import Annotated, Dict, List, Sequence, TypedDict, DefaultDict, Any, Optional\n",
    "from typing_extensions import TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "if not logger.hasHandlers():\n",
    "    logger.setLevel(logging.INFO)\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter('%(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of LLMs and Embedding Models\n",
    "1. Llama Models\n",
    "2. Normal Models\n",
    "3. Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "CLAUDE_API_KEY = os.getenv('CLAUDE_API_KEY')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = CLAUDE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_llama = OpenAI(model=\"gpt-3.5-turbo-0125\", openai_api_key=OPENAI_API_KEY, temperature=0.0)\n",
    "claude_llama = Anthropic(model=\"claude-3-5-sonnet-20240620\", api_key=CLAUDE_API_KEY, temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude = ChatAnthropic(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    temperature=0.0,\n",
    "    stop=[\"\\n\\nHuman\"],\n",
    "    streaming=True,\n",
    "    stream_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bge_embed_model = TextEmbedding(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "openai_embed_model = OpenAIEmbedding(model_name=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define GraphState's Storage Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of a graph.\n",
    "\n",
    "    Attributes:\n",
    "        query (str): The user query\n",
    "        agent (str): The agent responsible for decision making/answer generating\n",
    "        contexts (DefaultDict[str, str]): The contexts retrieved. Keys are \"kg\" or \"db\" indicating the source of the context, and values are the contexts themselves.\n",
    "        metrics (DefaultDict[str, str]): The numerical evaluations of metrics, such as \"correctness\", \"relevance\", \"clarity\", etc.\n",
    "        reasons (DefaultDict[str, str]): The reasons for the the metrics. Keys are the metric names, and values are the reasons.\n",
    "        answer (str): The answer generated by the agent\n",
    "    \"\"\"\n",
    "    query: str\n",
    "    agent: str\n",
    "    contexts: DefaultDict[str, str]\n",
    "    metrics: DefaultDict[str, str]\n",
    "    reasons: DefaultDict[str, str]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Helper Functions\n",
    "1. process_query_kg (retrieves context from KG according to query)\n",
    "2. process_query_db (retrieves context from a traditional vector DB according to query)\n",
    "3. get_kg_context (uses parallel processing to retrieve all contexts required from a query list, and formats it -- kg)\n",
    "4. get_db_context (uses parallel processing to retrieve all contexts required from a query list, and formats it -- vector DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query_kg(query: str, retriever) -> str:\n",
    "    \"\"\"\n",
    "    Helper Function to retrieve context from KG a single query\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted context retrieved from the knowledge graph\n",
    "    \"\"\"\n",
    "    return f\"{retriever.get_context(query)}\"\n",
    "\n",
    "def process_query_db(query: str, retriever) -> str:\n",
    "    \"\"\"\n",
    "    Helper Function to retrieve context from KG a single query\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted context retrieved from the knowledge graph\n",
    "    \"\"\"\n",
    "    return f\"{retriever.get_context(query)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kg_context(queries: List[str], retriever) -> str:\n",
    "    \"\"\"\n",
    "    Uses parallel processing to retrieve context from the KG for a list of queries.\n",
    "\n",
    "    Args:\n",
    "        query_list (List[str]): The list of user queries\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted context retrieved from the knowledge graph\n",
    "    \"\"\"\n",
    "\n",
    "    context = []\n",
    "        \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        future_to_query = {executor.submit(process_query_kg, query, retriever): query for query in queries}\n",
    "        for future in as_completed(future_to_query):\n",
    "            query = future_to_query[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                context.append(result)\n",
    "            except Exception as exc:\n",
    "                print(f\"Query {query} generated an exception: {exc}\")\n",
    "\n",
    "    return \"\\n\\n\".join(context)\n",
    "\n",
    "def get_db_context(queries: List[str], retriever) -> str:\n",
    "    \"\"\"\n",
    "    Uses parallel processing to retrieve context from a Vector DB for a list of queries.\n",
    "\n",
    "    Args:\n",
    "        queries (List[str]): The list of user queries\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted context retrieved from the DB\n",
    "    \"\"\"\n",
    "    \n",
    "    context = []\n",
    "        \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        future_to_query = {executor.submit(process_query_db, query, retriever): query for query in queries}\n",
    "        for future in as_completed(future_to_query):\n",
    "            query = future_to_query[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                context.append(result)\n",
    "            except Exception as exc:\n",
    "                print(f\"Query {query} generated an exception: {exc}\")\n",
    "\n",
    "    return \"\\n\\n\".join(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Agents\n",
    "1. Retrieval Agent (retrieves from kg_db and vector_db)\n",
    "2. Answer Generation Agent (uses context from vector_db to generate answer)\n",
    "3. Grading Agent (grades previously generated answer)\n",
    "4. Display Subgraph Agent (multi hops for context expansion)\n",
    "5. Refine Answer Agent (uses context retrieved from kg_db to refine previous answer)\n",
    "6. Regrading Agent (regrades the final answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_expansion(query: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Expands a query using LLM\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query\n",
    "\n",
    "    Returns:\n",
    "        List[str]: The expanded queries\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = \"\"\"<system>\n",
    "    You are a creative AI assistant specializing in expanding user queries to make them more comprehensive and diverse. Your goal is to generate multiple variant queries based on the initial user query, capturing different aspects, synonyms, related terms, and broader or narrower contexts. Ensure that the expanded queries are relevant, diverse, and avoid repetition.\n",
    "    </system>\n",
    "\n",
    "    <instructions>\n",
    "    1. Take the initial query provided by the user.\n",
    "    2. Generate 3 variant queries that explore different interpretations, related topics, or alternative phrasings.\n",
    "    3. Ensure the variants cover a range of specific to broad scopes and use synonyms or related terms.\n",
    "    4. Avoid repeating the same information or using overly similar phrasing.\n",
    "    5. Output the expanded queries in a JSON format, following the examples provided.\n",
    "    6. Do not include any preamble, explanation, or additional information beyond the expanded queries in the given JSON format.\n",
    "    </instructions>\n",
    "\n",
    "    <example_output>\n",
    "    Query: \"machine learning algorithms\"\n",
    "    {{\n",
    "        \"expanded_queries\": [\n",
    "            \"types of machine learning algorithms\",\n",
    "            \"applications of supervised learning techniques\",\n",
    "            \"deep learning vs traditional machine learning approaches\"\n",
    "        ]\n",
    "    }}\n",
    "    <example_output>\n",
    "\n",
    "    <query>\n",
    "    {query}\n",
    "    </query>\n",
    "    \n",
    "    <response>\n",
    "    [Your response here]\n",
    "    </response>\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"query\"],\n",
    "        template=prompt\n",
    "    )\n",
    "    \n",
    "    chain = prompt_template | claude | JsonOutputParser()\n",
    "    query_list = chain.invoke({\"query\": query})\n",
    "    return query_list[\"expanded_queries\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_expansion_tool = StructuredTool.from_function(\n",
    "    func=query_expansion,\n",
    "    name=\"Query Expansion\",\n",
    "    description=\"Expands a query using LLM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_expansion_agent(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Agent that expands the user query\n",
    "\n",
    "    Args:\n",
    "        state (GraphState): The state of the graph\n",
    "\n",
    "    Returns:\n",
    "        GraphState: The updated state of the graph\n",
    "    \"\"\"\n",
    "    expanded_queries = query_expansion_tool.invoke(state[\"query\"])\n",
    "    return {\"expanded_queries\": expanded_queries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_kg(queries: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves context from the ingested knowledge graph, given a list of queries.\n",
    "    Processes queries in parallel using ThreadPoolExecutor.\n",
    "\n",
    "    Args:\n",
    "        queries (List[str]): List of queries generated previously by the agent\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted context retrieved from the knowledge graph\n",
    "    \"\"\"\n",
    "    \n",
    "    kg_retriever = ...\n",
    "    context = get_kg_context(queries=queries, retriever=kg_retriever)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_kg_tool = StructuredTool.from_function(\n",
    "    func=retrieve_kg,\n",
    "    name=\"Retrieve Knowledge Graph\",\n",
    "    description=\"Retrieves context from the ingested knowledge graph, given a list of queries\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_kg_agent(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Agent that retrieves context from the ingested knowledge graph\n",
    "\n",
    "    Args:\n",
    "        state (GraphState): The state of the graph\n",
    "\n",
    "    Returns:\n",
    "        GraphState: The updated state of the graph\n",
    "    \"\"\"\n",
    "    context = retrieve_kg_tool.invoke(state[\"expanded_queries\"] + [state[\"query\"]])\n",
    "    return {state[\"contexts\"][\"kg\"]: context}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_db(queries: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves context from a conventional Vector DB, given a list of queries.\n",
    "\n",
    "    Args:\n",
    "        queries (List[str]): List of queries generated previously by the agent\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted context retrieved from Vector DB\n",
    "    \"\"\"\n",
    "    \n",
    "    db_retriever = ...\n",
    "    context = get_db_context(queries=queries, retriever=db_retriever)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_db_tool = StructuredTool.from_function(\n",
    "    func=retrieve_db,\n",
    "    name=\"Retrieve Vector DB\",\n",
    "    description=\"Retrieves context from a conventional Vector DB, given a list of queries.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_db_agent(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Agent that retrieves context from a Vector DB\n",
    "\n",
    "    Args:\n",
    "        state (GraphState): The state of the graph\n",
    "\n",
    "    Returns:\n",
    "        GraphState: The updated state of the graph\n",
    "    \"\"\"\n",
    "    context = retrieve_db_tool.invoke(state[\"expanded_queries\"] + [state[\"query\"]])\n",
    "    return {state[\"contexts\"][\"db\"]: context}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates an answer to the user query from the Vector DB context\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query\n",
    "        context (str): The context retrieved from the Vector DB\n",
    "\n",
    "    Returns:\n",
    "        str: The answer generated by the agent\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = \"\"\"<system>\n",
    "    You are an AI assistant that specializes in generating answers based on the provided context. Your goal is to provide a concise and informative response to the user's query by extracting relevant information from the given context.\n",
    "    </system>\n",
    "\n",
    "    <instruction>\n",
    "    1. Understand the Query: Carefully read and understand the user's query to identify the key information required.\n",
    "    2. Extract Relevant Information: Identify the most relevant parts of the provided context that directly answer the user's query.\n",
    "    3. Conciseness and Clarity: Generate a response that is clear, concise, and directly addresses the user's question without including unnecessary information.\n",
    "    4. Completeness: Ensure the answer covers all aspects of the query as much as possible based on the provided context.\n",
    "    5. Neutral and Informative Tone: Provide the answer in a neutral, professional tone, ensuring factual accuracy.\n",
    "    6. Stay Direct and Focused: Provide a straightforward answer without any introductory remarks, elaborations, or additional comments that do not pertain to the query.\n",
    "    </instruction>\n",
    "    \n",
    "    <query>\n",
    "    {query}\n",
    "    </query>\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    <response>\n",
    "    [Your response here]\n",
    "    </response>\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"query\", \"context\"],\n",
    "        template=prompt\n",
    "    )\n",
    "    \n",
    "    chain = prompt_template | claude | StrOutputParser()\n",
    "    response = chain.invoke({\"query\": query, \"context\": context})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answer_tool = StructuredTool.from_function(\n",
    "    func=generate_answer,\n",
    "    name=\"DB Answer Generator\",\n",
    "    description=\"Generates an answer to the user query from the Vector DB context\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_agent(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Agent that generates an answer to the user query from the Vector DB context\n",
    "\n",
    "    Args:\n",
    "        state (GraphState): The state of the graph\n",
    "\n",
    "    Returns:\n",
    "        state (GraphState): The updated state of the graph\n",
    "    \"\"\"\n",
    "    query, context = state[\"query\"], state[\"contexts\"][\"db\"]\n",
    "    response = generate_answer_tool.invoke({\"query\": query, \"context\": context})\n",
    "    return {state[\"answer\"]: response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grader(query: str, answer: str) -> str:\n",
    "    \"\"\"\n",
    "    Grades the answers based on Answer Relevancy, Completeness,  to the query\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query\n",
    "\n",
    "    Returns:\n",
    "        str: The preferred context based on the grading criteria\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = \"\"\"<system>\n",
    "    You are an expert evaluator specializing in assessing the quality of text responses based on specific criteria. Your task is to evaluate a given answer to a query and provide a detailed assessment and scoring for each of the criteria outlined below.\n",
    "    </system>\n",
    "\n",
    "    <instructions>\n",
    "    - Evaluation Criteria: Assess the provided answer according to the following four metrics: \n",
    "    1. Answer Relevancy: Evaluate how closely the answer addresses the query. Consider whether the answer is directly relevant, partially relevant, or irrelevant.\n",
    "    2. Completeness: Assess whether the answer provides a comprehensive response. Does it cover all necessary aspects of the topic in sufficient detail, or are there important points missing?\n",
    "    3. Clarity and Coherence: Review the answer for logical flow, readability, and overall coherence. Is the information presented in a clear and organized manner, or are there confusing or disjointed elements?\n",
    "    4. Correctness: Check the factual accuracy of the information provided. Are there any inaccuracies, errors, or misleading statements in the answer?\n",
    "\n",
    "    - Scoring System: Assign a numerical score from 1 to 10 for each metric, where 1 is the lowest and 10 is the highest. Scores must be integers without decimal values.\n",
    "\n",
    "    - Detailed Explanation: For each metric, provide a detailed explanation justifying the assigned score. The explanation should refer to specific parts of the answer and use evidence-based reasoning to support the score given.\n",
    "\n",
    "    - Output Format: Return your evaluation in the exact JSON format provided below. Ensure that all fields are correctly filled and formatted as specified.\n",
    "\n",
    "    - Formatting Guidelines: Do not include any additional information, preamble, or explanations beyond what is asked. Adhere strictly to the output format to ensure consistency.\n",
    "    </instructions>\n",
    "\n",
    "    <metrics>\n",
    "    1. Answer Relevancy: How directly does the answer address the query provided?\n",
    "    2. Completeness: Does the answer provide a full and detailed response to the question?\n",
    "    3. Clarity and Coherence: Is the answer clearly written, well-organized, and easy to follow?\n",
    "    4. Correctness: Are all the statements in the answer factually correct and free from errors?\n",
    "    </metrics>\n",
    "\n",
    "    <query>\n",
    "    {query}\n",
    "    </query>\n",
    "\n",
    "    <answer>\n",
    "    {answer}\n",
    "    </answer>\n",
    "\n",
    "    <output_format>\n",
    "    {{\n",
    "        \"evaluation\": {{\n",
    "            \"relevance\": 10,\n",
    "            \"completeness\": 9,\n",
    "            \"coherence\": 8,\n",
    "            \"correctness\": 7\n",
    "        }},\n",
    "        \"reasoning\": {{\n",
    "            \"relevance\": \"The answer is highly relevant to the query, directly addressing the question with a specific focus on the required topics.\",\n",
    "            \"completeness\": \"While the answer covers most necessary aspects, it lacks detail on a few minor points, which could enhance the response.\",\n",
    "            \"coherence\": \"The answer is generally clear and follows a logical structure, but there are a few sentences that could be more concise.\",\n",
    "            \"correctness\": \"There are some inaccuracies in the data provided, particularly concerning the explanation of key terms.\"\n",
    "        }}\n",
    "    }}\n",
    "    </output_format>\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"query\", \"context_kg\", \"context_db\"],\n",
    "        template=prompt\n",
    "    )\n",
    "    \n",
    "    chain = prompt_template | claude | JsonOutputParser()\n",
    "    result = chain.invoke({\"query\": query, \"answer\": answer})\n",
    "    return result[\"evaluation\"], result[\"reasoning\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader_tool = StructuredTool.from_function(\n",
    "    func=grader,\n",
    "    name=\"Grader\",\n",
    "    description=\"Grades the contexts retrieved from KG and conventional sources based on relevance and quality to the query\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grader_agent(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Agent that grades the contexts retrieved from KG and conventional sources\n",
    "\n",
    "    Args:\n",
    "        state (GraphState): The state of the graph\n",
    "\n",
    "    Returns:\n",
    "        GraphState: The updated state of the graph\n",
    "    \"\"\"\n",
    "    \n",
    "    query, answer = state[\"query\"], state[\"answer\"]\n",
    "    evaluation, reasoning = grader_tool.invoke({\"query\":query, \"answer\": answer})\n",
    "    relevance, completeness, coherence, correctness = evaluation[\"relevance\"], evaluation[\"completeness\"], evaluation[\"coherence\"], evaluation[\"correctness\"]\n",
    "    reason_relevance, reason_completeness, reason_coherence, reason_correctness = reasoning[\"relevance\"], reasoning[\"completeness\"], reasoning[\"coherence\"], reasoning[\"correctness\"]\n",
    "    return {\n",
    "        state[\"metrics\"][\"relevance\"]: relevance,\n",
    "        state[\"metrics\"][\"completeness\"]: completeness,\n",
    "        state[\"metrics\"][\"coherence\"]: coherence,\n",
    "        state[\"metrics\"][\"correctness\"]: correctness,\n",
    "        state[\"reasons\"][\"relevance\"]: reason_relevance,\n",
    "        state[\"reasons\"][\"completeness\"]: reason_completeness,\n",
    "        state[\"reasons\"][\"coherence\"]: reason_coherence,\n",
    "        state[\"reasons\"][\"correctness\"]: reason_correctness\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_metrics_agent(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Checks the metrics of the answer generated and decides if they are good enough\n",
    "\n",
    "    Args:\n",
    "        state (GraphState): The state of the graph\n",
    "\n",
    "    Returns:\n",
    "        GraphState: The updated state of the graph\n",
    "    \"\"\"\n",
    "    \n",
    "    metric_list = [state[\"metrics\"][\"relevance\"], state[\"metrics\"][\"completeness\"], state[\"metrics\"][\"coherence\"], state[\"metrics\"][\"correctness\"]]\n",
    "    for metric in metric_list:\n",
    "        metric = str(metric)\n",
    "        if metric <= 5:\n",
    "            \"good\"\n",
    "    return \"not good enough\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_answer(query: str, answer: str, kg_context) -> str:\n",
    "    \"\"\"\n",
    "    Refines the initial answer using context retrieved from the KG\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query \n",
    "        answer (str): The initial answer generated\n",
    "        kg_context (str): The context retrieved from the KG\n",
    "\n",
    "    Returns:\n",
    "        str: The refined answer\n",
    "    \"\"\"\n",
    "    prompt = \"\"\"<system>\n",
    "    You are an expert answer refiner with access to both the initial answer and additional context from a Knowledge Graph (KG). Your task is to enhance the initial answer using the additional context provided while maintaining logical coherence, relevance, completeness, and correctness. Ensure that the refined answer is comprehensive, factually accurate, and directly addresses the query.\n",
    "    </system>\n",
    "\n",
    "    <instructions>\n",
    "    - Review the initial answer provided in response to the query. Identify areas where the answer could be more detailed, accurate, or relevant.\n",
    "    - Incorporate relevant information from the KG context to improve the answer. Ensure that the added information directly supports or expands upon the initial answer without deviating from the main topic of the query.\n",
    "    - Maintain a clear and logical flow in the refined answer. Avoid redundancy and ensure that the enhanced content is seamlessly integrated with the existing text.\n",
    "    - The refined answer should be concise yet comprehensive, covering all aspects of the query as fully as possible with the available context.\n",
    "    - Ensure that all statements in the refined answer are factually accurate and derived from either the initial answer or the KG context. Avoid introducing unsupported or speculative information.\n",
    "    - Do not include any preamble or additional commentary. Return only the refined answer text in a natural and fluent style.\n",
    "    </instructions>\n",
    "\n",
    "    <query>\n",
    "    {query}\n",
    "    </query>\n",
    "\n",
    "    <initial_answer>\n",
    "    {answer}\n",
    "    </initial_answer>\n",
    "\n",
    "    <context>\n",
    "    {kg_context}\n",
    "    </context>\n",
    "\n",
    "    <refined_answer>\n",
    "    [Your refined answer here]\n",
    "    </refined_answer>\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Dummy agent that does nothing\n",
    "\n",
    "    Args:\n",
    "        state (GraphState): The state of the graph\n",
    "\n",
    "    Returns:\n",
    "        GraphState: Unchanged state of the graph\n",
    "    \"\"\"\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(GraphState)\n",
    "\n",
    "builder.add_node(\"search_kg_db\", retrieve_kg_agent)\n",
    "builder.add_node(\"search_vector_db\", retrieve_db_agent)\n",
    "builder.add_node(\"generate_answer\", generate_answer_agent)\n",
    "builder.add_node(\"grader\", grader_agent)\n",
    "\n",
    "# dummy nodes for now\n",
    "builder.add_node(\"display_subgraphs\", dummy)\n",
    "builder.add_node(\"subgraph_1\", dummy)\n",
    "builder.add_node(\"subgraph_2\", dummy)\n",
    "builder.add_node(\"refine_answer\", dummy)\n",
    "\n",
    "builder.set_entry_point(START)\n",
    "\n",
    "# retrieve both in parallel\n",
    "builder.add_edge(START, \"search_kg_db\")\n",
    "builder.add_edge(START, \"search_vector_db\")\n",
    "\n",
    "builder.add_edge([\"search_kg_db\", \"search_vector_db\"], \"generate_answer\")\n",
    "builder.add_edge(\"generate_answer\", \"grader\")\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"grader\",\n",
    "    decide_metrics_agent,\n",
    "    {\n",
    "        \"good\": END,\n",
    "        \"not good enough\": \"display_subgraphs\"\n",
    "    }\n",
    ")\n",
    "\n",
    "builder.add_edge(\"display_subgraphs\", [\"subgraph_1\", \"subgraph_2\"])\n",
    "builder.add_edge([\"subgraph_1\", \"subgraph_2\"], \"refine_answer\")\n",
    "builder.add_edge(\"refine_answer\", \"grader\")\n",
    "builder.add_edge(\"grader\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_png = graph.get_graph().draw_mermaid_png(\n",
    "    curve_style=CurveStyle.LINEAR,\n",
    "    wrap_label_n_words=4,\n",
    "    output_file_path=None,\n",
    "    draw_method=MermaidDrawMethod.API,\n",
    "    background_color=\"#000000\",\n",
    "    padding=10,\n",
    ")\n",
    "\n",
    "graph_base64 = base64.b64encode(graph_png).decode(\"utf-8\")\n",
    "\n",
    "HTML(f'''\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"data:image/png;base64,{graph_base64}\" width=\"400\" height=\"auto\"/>\n",
    "</div>\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
